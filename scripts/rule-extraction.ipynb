{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_conforming_numerical_features = pd.read_pickle(\"clustered_non_comforming_numerical.pkl\")\n",
    "non_conforming_categorical_features = pd.read_pickle(\"clustered_non_comforming_categorical.pkl\")\n",
    "\n",
    "conforming_numerical_features = pd.read_pickle(\"conforming_numerical_features.pkl\")\n",
    "conforming_categorical_features = pd.read_pickle(\"conforming_categorical_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conforming_numerical_features = conforming_numerical_features.drop(columns='case:concept:name')\n",
    "conforming_categorical_features = conforming_categorical_features.drop(columns='case:concept:name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Cluster Value to Conforming Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conforming_numerical_features[\"cluster\"] = 3\n",
    "conforming_categorical_features[\"cluster\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = pd.concat([non_conforming_numerical_features.drop(\"cluster\", axis=1),\n",
    "                    conforming_numerical_features.drop(\"cluster\", axis=1)], axis=0)\n",
    "\n",
    "y_num = pd.concat([non_conforming_numerical_features[\"cluster\"],\n",
    "                    conforming_numerical_features[\"cluster\"]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules = pd.concat([non_conforming_categorical_features,\n",
    "                    conforming_categorical_features], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state=42)\n",
    "model.fit(X_num, y_num)\n",
    "\n",
    "y_pred = model.predict(X_num)\n",
    "\n",
    "feature_names = X_num.columns\n",
    "importances = model.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "importance_df.head(15).plot(kind='barh', x='feature', y='importance', legend=False)\n",
    "plt.title(\"Top 15 Most Important Features (XGBoost)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_num)\n",
    "\n",
    "shap.summary_plot(shap_values, X_num, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4)\n",
    "tree.fit(X_num, y_num)\n",
    "\n",
    "print(export_text(tree, feature_names=list(X_num.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rules[\"cluster\"] = df_rules[\"cluster\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "cluster_counts = df_rules['cluster'].explode().value_counts()\n",
    "print(\"Cluster distribution in consequents:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "cluster_3_data = df_rules[df_rules['cluster'] == '3']\n",
    "other_clusters_data = df_rules[df_rules['cluster'] != '3']\n",
    "\n",
    "desired_size = cluster_counts.min()\n",
    "\n",
    "undersampled_cluster_3 = resample(cluster_3_data, \n",
    "                                  replace=False,  \n",
    "                                  n_samples=desired_size, \n",
    "                                  random_state=42)\n",
    "\n",
    "balanced_df_rules = pd.concat([undersampled_cluster_3, other_clusters_data])\n",
    "\n",
    "new_cluster_counts = balanced_df_rules['cluster'].explode().value_counts()\n",
    "print(\"New cluster distribution in consequents:\")\n",
    "print(new_cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(balanced_df_rules)\n",
    "\n",
    "min_support_threshold_for_filtering = 0.05 \n",
    "\n",
    "item_support = df_encoded.sum() / len(df_encoded)\n",
    "\n",
    "items_to_keep = item_support[item_support >= min_support_threshold_for_filtering].index\n",
    "print(f\"Original number of items: {df_encoded.shape[1]}\")\n",
    "print(f\"Number of items after filtering: {len(items_to_keep)}\")\n",
    "\n",
    "df_filtered = df_encoded[items_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "frequent_itemsets = fpgrowth(df_filtered, min_support=0.1, use_colnames=True, max_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consequent-Based Association Rule Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_suffixes = (\"absent\", \"Never\")\n",
    "\n",
    "def contains_excluded_suffix(itemset):\n",
    "  if not itemset:\n",
    "    return False\n",
    "  for item in itemset:\n",
    "    if isinstance(item, str) and item.endswith(excluded_suffixes):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "rows_to_remove_mask = rules['antecedents'].apply(contains_excluded_suffix) | \\\n",
    "                      rules['consequents'].apply(contains_excluded_suffix)\n",
    "\n",
    "rules_filtered = rules[~rows_to_remove_mask]\n",
    "\n",
    "print(f\"Original number of rules: {len(rules)}\")\n",
    "print(f\"Number of rules after filtering suffixes {excluded_suffixes}: {len(rules_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_consequents = rules['consequents'].apply(\n",
    "    lambda x: {item for item in x if \"cluster\" in str(item)}\n",
    ").explode().dropna().unique()\n",
    "\n",
    "print(\"Unique consequents containing 'cluster':\")\n",
    "for consequent in unique_consequents:\n",
    "    print(consequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_clusters = rules_filtered[rules_filtered['consequents'].apply(\n",
    "    lambda x: len(x) == 1 and list(x)[0].startswith('cluster_')\n",
    ")].copy()\n",
    "\n",
    "rules_clusters = rules_clusters.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of rules after filtering exclusive cluster consequents: {len(rules_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_clusters['consequents'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_cluster_consequent(x, cluster_name):\n",
    "    return len(x) == 1 and cluster_name in x\n",
    "\n",
    "rules_cluster_0 = rules_clusters[rules_clusters['consequents'].apply(\n",
    "    lambda x: is_single_cluster_consequent(x, 'cluster_0')\n",
    ")].copy()\n",
    "\n",
    "rules_cluster_1 = rules_clusters[rules_clusters['consequents'].apply(\n",
    "    lambda x: is_single_cluster_consequent(x, 'cluster_1')\n",
    ")].copy()\n",
    "\n",
    "rules_cluster_2 = rules_clusters[rules_clusters['consequents'].apply(\n",
    "    lambda x: is_single_cluster_consequent(x, 'cluster_2')\n",
    ")].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools \n",
    "\n",
    "TOP_N_RULES_FOR_PLOTS = 50    \n",
    "TOP_M_FEATURES_FOR_BAR = 15  \n",
    "TOP_M_FEATURES_FOR_HEATMAP = 10 \n",
    "SORT_BY_METRIC = 'lift'       \n",
    "\n",
    "try:\n",
    "    rules_cluster_0.head()\n",
    "    rules_cluster_1.head()\n",
    "    rules_cluster_2.head()\n",
    "    print(\"Using existing rules DataFrames.\")\n",
    "\n",
    "    if 'cluster' not in rules_cluster_0.columns:\n",
    "      rules_cluster_0 = rules_cluster_0.assign(cluster='Cluster_0')\n",
    "    if 'cluster' not in rules_cluster_1.columns:\n",
    "      rules_cluster_1 = rules_cluster_1.assign(cluster='Cluster_1')\n",
    "    if 'cluster' not in rules_cluster_2.columns:\n",
    "      rules_cluster_2 = rules_cluster_2.assign(cluster='Cluster_2')\n",
    "\n",
    "except NameError:\n",
    "    print(\"Creating dummy rules DataFrames for demonstration.\")\n",
    "    dummy_data = {\n",
    "        'antecedents': [frozenset({f'feature_{i}', f'feature_{j}'}) for i in range(5) for j in range(i+1, 6)],\n",
    "        'consequents': [frozenset({'Cluster_X'})] * 10, \n",
    "        'support': np.random.rand(10) * 0.1,\n",
    "        'confidence': np.random.rand(10) * 0.4 + 0.6,\n",
    "        'lift': np.random.rand(10) * 3 + 1,\n",
    "    }\n",
    "    rules_cluster_0 = pd.DataFrame(dummy_data)\n",
    "    rules_cluster_0['consequents'] = rules_cluster_0['consequents'].apply(lambda x: frozenset({'Cluster_0'}))\n",
    "    rules_cluster_0['cluster'] = 'Cluster_0'\n",
    "\n",
    "    rules_cluster_1 = pd.DataFrame(dummy_data)\n",
    "    rules_cluster_1['antecedents'] = rules_cluster_1['antecedents'].apply(lambda s: frozenset({item.replace('feature_','feat_') for item in s})) \n",
    "    rules_cluster_1['consequents'] = rules_cluster_1['consequents'].apply(lambda x: frozenset({'Cluster_1'}))\n",
    "    rules_cluster_1['confidence'] = np.random.rand(10) * 0.3 + 0.5 \n",
    "    rules_cluster_1['lift'] = np.random.rand(10) * 2 + 1.5 \n",
    "    rules_cluster_1['cluster'] = 'Cluster_1'\n",
    "\n",
    "    rules_cluster_2 = pd.DataFrame(dummy_data)\n",
    "    rules_cluster_2['antecedents'] = rules_cluster_2['antecedents'].apply(lambda s: frozenset({item.replace('feature_','item_') for item in s}))\n",
    "    rules_cluster_2['consequents'] = rules_cluster_2['consequents'].apply(lambda x: frozenset({'Cluster_2'}))\n",
    "    rules_cluster_2['support'] = np.random.rand(10) * 0.05 \n",
    "    rules_cluster_2['lift'] = np.random.rand(10) * 1 + 1 \n",
    "    rules_cluster_2['cluster'] = 'Cluster_2'\n",
    "\n",
    "\n",
    "cluster_rules_dfs = {\n",
    "    \"Cluster_0\": rules_cluster_0,\n",
    "    \"Cluster_1\": rules_cluster_1,\n",
    "    \"Cluster_2\": rules_cluster_2,\n",
    "}\n",
    "\n",
    "def plot_feature_frequency(rules_df, cluster_name, top_n_rules, top_m_features, sort_by):\n",
    "    \"\"\"Plots a bar chart of the most frequent features in rule antecedents.\"\"\"\n",
    "    if rules_df.empty:\n",
    "        print(f\"No rules for {cluster_name} to plot feature frequency.\")\n",
    "        return\n",
    "\n",
    "    top_rules = rules_df.sort_values(by=sort_by, ascending=False).head(top_n_rules)\n",
    "    if top_rules.empty:\n",
    "        print(f\"Not enough rules for {cluster_name} after sorting/selecting top {top_n_rules}.\")\n",
    "        return\n",
    "\n",
    "    antecedent_items = list(itertools.chain.from_iterable(top_rules['antecedents']))\n",
    "    item_counts = Counter(antecedent_items)\n",
    "\n",
    "    if not item_counts:\n",
    "        print(f\"No antecedent items found in the top rules for {cluster_name}.\")\n",
    "        return\n",
    "\n",
    "    features_df = pd.DataFrame(item_counts.items(), columns=['feature', 'count'])\n",
    "    features_df = features_df.sort_values(by='count', ascending=False).head(top_m_features)\n",
    "\n",
    "    plt.figure(figsize=(10, max(5, len(features_df)*0.4)))\n",
    "    sns.barplot(x='count', y='feature', data=features_df, hue='feature', palette='viridis', legend=False)\n",
    "    plt.title(f'Top {len(features_df)} Most Frequent Features in Antecedents\\n(Top {top_n_rules} Rules for {cluster_name}, sorted by {sort_by})')\n",
    "    plt.xlabel('Frequency in Top Rule Antecedents')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rule_metrics_scatter(rules_df, cluster_name, top_n_rules, sort_by):\n",
    "    \"\"\"Plots a scatter plot of rule metrics (Support vs Confidence, colored by Lift).\"\"\"\n",
    "    if rules_df.empty:\n",
    "        print(f\"No rules for {cluster_name} to plot scatter metrics.\")\n",
    "        return\n",
    "\n",
    "    top_rules = rules_df.sort_values(by=sort_by, ascending=False).head(top_n_rules)\n",
    "    if top_rules.empty:\n",
    "        print(f\"Not enough rules for {cluster_name} after sorting/selecting top {top_n_rules}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = sns.scatterplot(\n",
    "        data=top_rules,\n",
    "        x='support',\n",
    "        y='confidence',\n",
    "        hue='lift',\n",
    "        size='lift',  \n",
    "        palette='magma',\n",
    "        sizes=(20, 200), \n",
    "        legend='auto'\n",
    "    )\n",
    "    plt.title(f'Rule Metrics for {cluster_name}\\n(Top {len(top_rules)} Rules sorted by {sort_by})')\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.legend(title='Lift', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "\n",
    "def display_top_rules_table(rules_df, cluster_name, top_n_rules, sort_by):\n",
    "    \"\"\"Prints the top N rules in a formatted way.\"\"\"\n",
    "    if rules_df.empty:\n",
    "        print(f\"No rules for {cluster_name} to display.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Top {top_n_rules} Rules for {cluster_name} (Sorted by {sort_by}) ---\")\n",
    "    top_rules = rules_df.sort_values(by=sort_by, ascending=False).head(top_n_rules)\n",
    "\n",
    "    if top_rules.empty:\n",
    "        print(f\"Not enough rules found after sorting/selecting top {top_n_rules}.\")\n",
    "        return\n",
    "\n",
    "    print(top_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_string())\n",
    "    print(\"-\" * (len(cluster_name) + 30))\n",
    "\n",
    "\n",
    "def plot_metric_distributions(all_rules_list, cluster_names):\n",
    "    if not all_rules_list:\n",
    "        print(\"No rules data provided for distribution plotting.\")\n",
    "        return\n",
    "\n",
    "    combined_rules = pd.concat(all_rules_list, ignore_index=True)\n",
    "\n",
    "    if combined_rules.empty:\n",
    "        print(\"Combined rules DataFrame is empty. Cannot plot distributions.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x='cluster', y='confidence', data=combined_rules, palette='Set2', order=cluster_names)\n",
    "    plt.title('Confidence Distribution by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x='cluster', y='lift', data=combined_rules, palette='Set2', order=cluster_names)\n",
    "    plt.title('Lift Distribution by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "    plt.suptitle('Comparison of Rule Metric Distributions')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGenerating visualizations per cluster...\")\n",
    "for cluster_name, rules_df in cluster_rules_dfs.items():\n",
    "    print(f\"\\n--- {cluster_name} ---\")\n",
    "    display_top_rules_table(rules_df, cluster_name, TOP_N_RULES_FOR_PLOTS, SORT_BY_METRIC)\n",
    "    plot_feature_frequency(rules_df, cluster_name, TOP_N_RULES_FOR_PLOTS, TOP_M_FEATURES_FOR_BAR, SORT_BY_METRIC)\n",
    "    plot_rule_metrics_scatter(rules_df, cluster_name, TOP_N_RULES_FOR_PLOTS, SORT_BY_METRIC)\n",
    "    plot_feature_cooccurrence_heatmap(rules_df, cluster_name, TOP_N_RULES_FOR_PLOTS, TOP_M_FEATURES_FOR_HEATMAP, SORT_BY_METRIC)\n",
    "\n",
    "print(\"\\nGenerating comparison visualizations...\")\n",
    "all_rules_list = [df for df in cluster_rules_dfs.values() if not df.empty]\n",
    "cluster_names_list = [name for name, df in cluster_rules_dfs.items() if not df.empty]\n",
    "\n",
    "plot_metric_distributions(all_rules_list, cluster_names_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
